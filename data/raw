# src/data_ingest.py
#
# Purpose:
# - Download CIFAR-10 via torchvision
# - Export samples as PNG images into data/raw/{train,val,test}/{class}/
# - Create deterministic splits
# - Write JSON manifests and dataset statistics
# - Provide CLI to run end-to-end
#
# Usage:
#   python -m src.data_ingest --dataset cifar10 --data-root data/raw --val-ratio 0.1 --test-ratio 0.1 --seed 42
#
# Notes:
# - Designed to be extended to other datasets later (e.g., STL10, Caltech-256).
# - Keeps raw images; additional preprocessing will occur into data/processed later.

import os
import json
import shutil
import random
import hashlib
from pathlib import Path
from typing import Dict, List, Tuple

import torch
from torchvision import datasets, transforms
from PIL import Image
from tqdm import tqdm
import argparse
import time


# ---------------------------
# Configuration defaults
# ---------------------------
DEFAULT_DATA_ROOT = "data/raw"
DEFAULT_DATASET = "cifar10"
DEFAULT_VAL_RATIO = 0.1
DEFAULT_TEST_RATIO = 0.1
DEFAULT_SEED = 42
DEFAULT_IMAGE_FORMAT = "png"
DEFAULT_NUM_WORKERS = 2


# ---------------------------
# Utility helpers
# ---------------------------
def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def clean_dir(path: Path) -> None:
    if path.exists():
        shutil.rmtree(path)
    path.mkdir(parents=True, exist_ok=True)


def hash_string(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()[:16]


def save_manifest(manifest_path: Path, items: List[Dict]) -> None:
    ensure_dir(manifest_path.parent)
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(items, f, indent=2)


def save_json(path: Path, obj: Dict) -> None:
    ensure_dir(path.parent)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2)


def now_ts() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S")


# ---------------------------
# Dataset loading
# ---------------------------
def load_cifar10(tmp_dir: Path, train: bool) -> Tuple[List[Image.Image], List[int], List[str]]:
    """
    Loads CIFAR-10 using torchvision and returns PIL Images, labels, and class names.
    """
    transform = transforms.Compose([
        transforms.ToTensor(),  # load as tensor; we will convert back to PIL to save
    ])
    ds = datasets.CIFAR10(root=str(tmp_dir), train=train, download=True, transform=transform)
    class_names = ds.classes  # list of 10 class names
    images: List[Image.Image] = []
    labels: List[int] = []

    # Convert tensors to PIL for saving as PNG
    for x, y in tqdm(ds, desc=f"Converting CIFAR-10 ({'train' if train else 'test'}) to PIL"):
        # x: tensor in [C, H, W], range [0,1]
        x = (x * 255).clamp(0, 255).byte()
        pil_img = Image.fromarray(x.permute(1, 2, 0).numpy())
        images.append(pil_img)
        labels.append(y)

    return images, labels, class_names


# ---------------------------
# Splitting logic
# ---------------------------
def stratified_split(
    labels: List[int],
    val_ratio: float,
    test_ratio: float,
    seed: int
) -> Tuple[List[int], List[int], List[int]]:
    """
    Returns indices for train/val/test with stratification by labels.
    """
    random.seed(seed)

    indices = list(range(len(labels)))
    by_class: Dict[int, List[int]] = {}
    for idx, y in enumerate(labels):
        by_class.setdefault(y, []).append(idx)

    train_idx, val_idx, test_idx = [], [], []
    for cls, idxs in by_class.items():
        random.shuffle(idxs)
        n = len(idxs)
        n_test = int(n * test_ratio)
        n_val = int(n * val_ratio)
        test_idx.extend(idxs[:n_test])
        val_idx.extend(idxs[n_test:n_test + n_val])
        train_idx.extend(idxs[n_test + n_val:])

    return train_idx, val_idx, test_idx


# ---------------------------
# Export to disk
# ---------------------------
def export_split(
    images: List[Image.Image],
    labels: List[int],
    class_names: List[str],
    split_indices: List[int],
    split_name: str,
    out_root: Path,
    image_format: str = DEFAULT_IMAGE_FORMAT
) -> List[Dict]:
    """
    Saves images for a given split under out_root/{split_name}/{class}/image_xxx.png
    Returns manifest items: {path, label, class_name}.
    """
    split_dir = out_root / split_name
    clean_dir(split_dir)

    # Prepare class subdirectories
    for cname in class_names:
        ensure_dir(split_dir / cname)

    manifest: List[Dict] = []
    for i in tqdm(split_indices, desc=f"Saving {split_name} images"):
        y = labels[i]
        cname = class_names[y]
        img = images[i]

        # Deterministic filename
        fname = f"{split_name}_{cname}_{i}_{hash_string(f'{split_name}:{cname}:{i}')}.{image_format}"
        out_path = split_dir / cname / fname

        img.save(out_path)
        manifest.append({
            "path": str(out_path),
            "label": int(y),
            "class_name": cname
        })

    return manifest


# ---------------------------
# Pipeline orchestration
# ---------------------------
def build_raw_dataset(
    dataset_name: str = DEFAULT_DATASET,
    data_root: str = DEFAULT_DATA_ROOT,
    val_ratio: float = DEFAULT_VAL_RATIO,
    test_ratio: float = DEFAULT_TEST_RATIO,
    seed: int = DEFAULT_SEED,
    num_workers: int = DEFAULT_NUM_WORKERS
) -> Dict:
    """
    Orchestrates download, split, export, and manifest writing for raw images.
    Returns a summary dictionary with counts and paths.
    """
    # Validate ratios
    if val_ratio < 0 or test_ratio < 0 or (val_ratio + test_ratio) >= 0.9:
        raise ValueError("Invalid split ratios. Ensure 0 <= ratios and val+test < 0.9 for sufficient train samples.")

    data_root_path = Path(data_root)
    tmp_dir = data_root_path / "_tmp_downloads"
    ensure_dir(tmp_dir)
    ensure_dir(data_root_path / "manifests")

    # Load dataset
    if dataset_name.lower() == "cifar10":
        train_images, train_labels, class_names = load_cifar10(tmp_dir, train=True)
        test_images, test_labels, test_class_names = load_cifar10(tmp_dir, train=False)
        assert class_names == test_class_names, "Class names mismatch between train and test."
        images = train_images + test_images
        labels = train_labels + test_labels
    else:
        raise NotImplementedError(f"Dataset '{dataset_name}' not implemented yet. Use 'cifar10'.")

    # Split indices
    total = len(images)
    train_idx, val_idx, test_idx = stratified_split(labels, val_ratio=val_ratio, test_ratio=test_ratio, seed=seed)

    # Export splits
    manifests_root = data_root_path / "manifests"
    train_manifest = export_split(images, labels, class_names, train_idx, "train", data_root_path)
    val_manifest = export_split(images, labels, class_names, val_idx, "val", data_root_path)
    test_manifest = export_split(images, labels, class_names, test_idx, "test", data_root_path)

    # Write manifests
    save_manifest(manifests_root / "train.json", train_manifest)
    save_manifest(manifests_root / "val.json", val_manifest)
    save_manifest(manifests_root / "test.json", test_manifest)

    # Stats
    stats = {
        "dataset": dataset_name,
        "class_names": class_names,
        "counts": {
            "train": len(train_manifest),
            "val": len(val_manifest),
            "test": len(test_manifest),
            "total": len(train_manifest) + len(val_manifest) + len(test_manifest),
        },
        "splits": {
            "val_ratio": val_ratio,
            "test_ratio": test_ratio
        },
        "seed": seed,
        "image_format": DEFAULT_IMAGE_FORMAT,
        "generated_at": now_ts(),
        "root": str(data_root_path)
    }
    save_json(data_root_path / "stats.json", stats)

    # Cleanup tmp downloads
    shutil.rmtree(tmp_dir, ignore_errors=True)

    return stats


# ---------------------------
# CLI
# ---------------------------
def parse_args():
    p = argparse.ArgumentParser(description="Build raw image dataset into data/raw from a public source.")
    p.add_argument("--dataset", type=str, default=DEFAULT_DATASET, help="Dataset to use (cifar10).")
    p.add_argument("--data-root", type=str, default=DEFAULT_DATA_ROOT, help="Output root directory for raw images.")
    p.add_argument("--val-ratio", type=float, default=DEFAULT_VAL_RATIO, help="Validation split ratio (0-1).")
    p.add_argument("--test-ratio", type=float, default=DEFAULT_TEST_RATIO, help="Test split ratio (0-1).")
    p.add_argument("--seed", type=int, default=DEFAULT_SEED, help="Random seed for deterministic splits.")
    p.add_argument("--num-workers", type=int, default=DEFAULT_NUM_WORKERS, help="Reserved for future parallel export.")
    return p.parse_args()


def main():
    args = parse_args()
    print(f"[INFO] Starting raw dataset build with: {vars(args)}")
    stats = build_raw_dataset(
        dataset_name=args.dataset,
        data_root=args.data_root,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        seed=args.seed,
        num_workers=args.num_workers
    )
    print("[INFO] Completed raw dataset build.")
    print(json.dumps(stats, indent=2))


if __name__ == "__main__":
    main()
